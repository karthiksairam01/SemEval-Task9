% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{assael2022restoring,
  title = {Restoring and attributing ancient texts using deep neural networks},
  author = {Assael, Yannis and Sommerschield, Thea and Shillingford, Brendan and others},
  journal = {Nature},
  volume = {603},
  pages = {280--283},
  year = {2022},
  doi = {10.1038/s41586-022-04448-z}
}


@inproceedings{pavlopoulos-etal-2022-detection,
    title = "From the Detection of Toxic Spans in Online Discussions to the Analysis of Toxic-to-Civil Transfer",
    author = "Pavlopoulos, John  and
      Laugier, Leo  and
      Xenos, Alexandros  and
      Sorensen, Jeffrey  and
      Androutsopoulos, Ion",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.259",
    doi = "10.18653/v1/2022.acl-long.259",
    pages = "3721--3734",
    
}

@inproceedings{ribeiro-etal-2016-trust,
    title = "{``}Why Should {I} Trust You?{''}: Explaining the Predictions of Any Classifier",
    author = "Ribeiro, Marco  and
      Singh, Sameer  and
      Guestrin, Carlos",
    editor = "DeNero, John  and
      Finlayson, Mark  and
      Reddy, Sravana",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-3020",
    doi = "10.18653/v1/N16-3020",
    pages = "97--101",
}
@INPROCEEDINGS{7881747,
  author={Hwon Ihm and Kyoungrok Jang and Kangwook Lee and Gwan Jang and Min-Gwan Seo and Kyoungah Han and Sung-Hyon Myaeng},
  booktitle={2017 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Multi-source food hazard event extraction for public health}, 
  year={2017},
  volume={},
  number={},
  pages={414-417},
  keywords={Hazards;Data mining;Social network services;Media;Feature extraction;Monitoring;Filtering;food hazard;event extraction;news;social media},
  doi={10.1109/BIGCOMP.2017.7881747}}


@article{10.1145/3529755,
author = {Zini, Julia El and Awad, Mariette},
title = {On the Explainability of Natural Language Processing Deep Models},
year = {2022},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3529755},
doi = {10.1145/3529755},
abstract = {Despite their success, deep networks are used as black-box models with outputs that are not easily explainable during the learning and the prediction phases. This lack of interpretability is significantly limiting the adoption of such models in domains where decisions are critical such as the medical and legal fields. Recently, researchers have been interested in developing methods that help explain individual decisions and decipher the hidden representations of machine learning models in general and deep networks specifically. While there has been a recent explosion of work on Explainable Artificial Intelligence (ExAI) on deep models that operate on imagery and tabular data, textual datasets present new challenges to the ExAI community. Such challenges can be attributed to the lack of input structure in textual data, the use of word embeddings that add to the opacity of the models and the difficulty of the visualization of the inner workings of deep models when they are trained on textual data.Lately, methods have been developed to address the aforementioned challenges and present satisfactory explanations on Natural Language Processing (NLP) models. However, such methods are yet to be studied in a comprehensive framework where common challenges are properly stated and rigorous evaluation practices and metrics are proposed.Motivated to democratize ExAI methods in the NLP field, we present in this work a survey that studies model-agnostic as well as model-specific explainability methods on NLP models. Such methods can either develop inherently interpretable NLP models or operate on pre-trained models in a post hoc manner. We make this distinction and we further decompose the methods into three categories according to what they explain: (1) word embeddings (input level), (2) inner workings of NLP models (processing level), and (3) modelsâ€™ decisions (output level). We also detail the different evaluation approaches interpretability methods in the NLP field. Finally, we present a case-study on the well-known neural machine translation in an appendix, and we propose promising future research directions for ExAI in the NLP field.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {103},
numpages = {31},
keywords = {ExAI, NLP, language models, transformers, neural machine translation, transparent embedding models, explaining decisions}
}